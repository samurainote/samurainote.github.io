---
title:  "Algorithms for Coding Test"
date:   2019-3-30
layout: single
author_profile: true
comments: true
tags:
---

https://www.dummies.com/programming/big-data/data-science/machine-learning-dummies-cheat-sheet/

![](https://image.slidesharecdn.com/deeplearning-thepastpresentandfutureofartificialintelligence-151205235804-lva1-app6891/95/deep-learning-the-past-present-and-future-of-artificial-intelligence-17-638.jpg?cb=1487990822)
> https://www.slideshare.net/LuMa921/deep-learning-the-past-present-and-future-of-artificial-intelligence?from_action=save

**Menu**

Warmup
Implementation
Strings
Sorting
Search
Graph Theory
Greedy
Dynamic Programming
Constructive Algorithms
Bit Manipulation
Recursion
Game Theory
NP Complete
Debugging


## 統計 vs. 機械学習 回帰が強い

統計モデルはデータ生成のメカニズムに確率を持ち込み、大抵の場合は未知の値を持つ解釈可能なパラメータ（例えば予測変数の影響（係数）、ターゲット変数（予測したい対象）の分布といったもの。）を持ちます。
統計モデルでよく使われるのは回帰モデルで、複数の予測変数の影響度を切り離して考えることができます。
機械学習は典型的な統計パラメーターを使わずに、アルゴリズム的なアプローチをとります。そして予測変数と結果の間にある関係には前提となる構造は特に要求されません。機械学習は一般的に、一つ一つの変数の影響を単体では捉えようとはしません。
機械学習はデータ生成のプロセスをモデルするのではなく、単純に手元にあるデータから学ぼうとします。
機械学習と統計モデルの違いでもっともわかりやすいのは、統計モデルは予測変数の影響度を足していくことに重きを置くにの対して、機械学習はふつうそうした影響度の足し上げには特別な注意を払わないということです。

## 線形分離可能とは？


# TL;DR

社員の退職時期分析
ロジスティック回帰から始めて、次にアンサンブル学習木（バギング、ブースティング）、最後にNeural Networksを試してください。

## 項目

- Task
- Keyword
- Pros
- Cons
- General Information


- 好みの分布
- データ量
- データの質（非構造化データ ）
- 計算スピード・コスト
- 説明可能性
- Parallel processing 並列処理
- 過学習（cons）
- 応用事例


## 多変量線形回帰

データの分布や傾向を線によって表現できる線形モデル

- Task
  - 回帰(連続値)

- Keyword
  - 最小二乗法
  - 最尤推定
  - 相関係数

- Pros
  - どの説明変数が目的変数に与える影響が大きいかがわかる

- Cons
  - 多重共線性を処理する必要がある

- General Information
  - 計算コスト: 低
  - 説明可能性: ○
  - 好みの分布: 正規分布
  - データ量: 少量が向いている
  - データの質: 連続値

## ロジスティック回帰

ベルヌーイ分布に従う変数の統計的回帰モデルの一種であり、二項分類が得意な線形モデル

- Task
  - バイナリ分類(マルチクラスも可能)

Keyword:
  - 発生確率（シグモイド関数:0~1）: 信頼区間
  - オッズ比: Aのオッズ比　A:勝ち/負け // B:勝ち/負け = A/B
  - 正則化: 正則化項

Pros:
  - 多重共線性を処理する必要がない
  - ほとんどの非線形特徴量を線形特徴量に簡単にエンジニアリングできる
  - オンライン勾配降下法: モデルを簡単に更新して新しいデータを取り込むことができる

Cons:
  - ソフトマージン：外れ値などに過剰反応しやすい
  - 過学習を防ぐために正則化が必要
  - カテゴリーデータをうまく処理できない

- General Information
  - 計算コスト: 低
  - 説明可能性: ○
  - 好みの分布: ベルヌーイ分布
  - データ量: 少量が向いている
  - データの質: 連続値


## SVM（サポートベクトルマシン）

教師あり学習を用いるパターン認識を可能とするロバストな非線形モデル

- Task
  - 分類、回帰（バイナリ分類がメイン）

Keyword:
  - マージン最大化: ハードマージン、ロバスト、過学習に強い
  - カーネル法: 次元追加（特徴ベクトルの内積をカーネル関数に置き換える）

Pros:
  - 非常に高次元なデータ（数十万次元）に対応（テキスト分類などに適す)
  - 欠損値(スパースなデータ)に強い
  - ロバスト性: 異常値とノイズを非常にうまく処理する
  - 多重共線性を気にしなくていい

Cons:
  - ハイパーパラメータが多いため、調整がめんどくさい
  - 訓練するには非効率的で、メモリを大量に消費
  - データ量が少ないものに適し、ビッグデータを用いたビジネスアプリケーションには向かない

- General Information
  - 損失関数: LRとは異なる損失関数（ヒンジ）を使用
  - 計算コスト: 高
  - 説明可能性: ×
  - 好みの分布: ?
  - データ量: 少ない、非常に大きなデータセット（数十万以上の例）を扱うのは得意でない
  - データの質: 連続値、カテゴリ

## 決定木

木構造を用いて分類や回帰を行うグリーディーな非線形モデル

- Task
  - 分類、回帰

Keyword:
  - エントロピー（小さい順から）: 各ノードの特徴を決める（argmax(分割前の不純度-分割後の不純度)）
  - 閾値:  0 if x > 0.22

Pros:
  - 目的変数に無関係な変数を自動で削除（ジニ係数）
  - 早い、精度が高い
  - 欠損値や外れ値に強い

Cons:
  - 木の深さを指定して、過学習を防ぐ必要がある
  - グリーディーで最適化までは保証しない

- General Information
  - 損失関数: ?
  - 計算コスト: 高
  - 説明可能性: ○
  - 好みの分布: ?
  - データ量: 少ない、非常に大きなデータセット（数十万以上の例）を扱うのは得意でない
  - データの質: カテゴリーデータ

## ランダムフォレスト

- 多数決(分類)と平均（回帰）
- ブートストラップ: 一部のデータ（not feature）で木を作る



長所：

Gradient Boosting Machine（2つのハイパーパラメータと3つのハイパーパラメータ）よりも調整がはるかに簡単です。
ほとんどの場合、SVMと同等以上のパフォーマンスを発揮します。
変数が欠けている不​​均一なデータセットをうまく処理する。
めったにあふれない。
短所：

予測時間が遅いため、高速データ処理にはあまり適していません。
データにレベル数の異なるカテゴリカル変数が含まれている場合、ランダムフォレストはよりレベルの高いものに有利に偏っています。したがって、ランダムフォレストからの可変重要度スコアは信頼できません。

長所：

- 回帰も分類もOK
- アンサンブル学習
- 巨大なデータセット（数百万の例と数百万の次元）を扱うことができます。
- 可読性： △

ランダムフォレスト
データのランダムサンプルを使用して各ツリーを独立してトレーニングするので、トレーニングされたモデルは単一の決定木よりも堅牢であり、過剰適合する可能性が低くなります。
2つのパラメータ：各ノードで選択される木の数と特徴の数。
並列または分散コンピューティングに適しています。
決定木よりも低い分類誤差とより良いfスコア。
SVMと同等かそれ以上のパフォーマンスを発揮しますが、人間にとって理解しやすいものです。
変数がない不均一なデータセットに適しています。
特徴の重要度を計算します
SVMよりも早くトレーニング


## 勾配ブースティング木

- 逐次的訓練
- エラー率とラベリング: 誤分類に重みをかける
- Adaboost
-

前の学習結果に基づいて次の訓練を行う

長所：

- 回帰も分類もOK
- アンサンブル学習
- 巨大なデータセット（数百万の例と数百万の次元）を扱うことができます。
- 非常に正確です。
- エラー率によるラベリング
- 分類と回帰の両方のタスクに使用できます。
- 損失関数の選択に柔軟性がたくさんあります。問題の特性に合わせて調整できます。
- 可読性： △

短所：

- 木は順番に造られるので、訓練が遅くなることがあります。
- モデルはブラックボックスです（すべてのアンサンブルメソッドに関して）。
- 過剰適合する傾向があります（ただし、ハイパーパラメーター調整は役立ちます）。

勾配ブースト決定木
一度に1つずつツリーを構築し、新しいツリーごとに前のツリーによって行われたエラーを修正します。モデルはさらに表現力を高めます。
3つのパラメータ - 木の数、木の深さ、および学習率。木は一般的に浅いです。
通常、ランダムフォレストよりもパフォーマンスは優れていますが、正しく実行するのは困難です。ハイパーパラメーターは調整が難しく、オーバーフィットする傾向があります。 RFは「箱から出して」ほとんど機能します。
木は順番に造られるので訓練はより長くかかる


## ナイーブベイズ

- 非常に単純なもの（数点を数えるだけ）で、実際にはうまく機能しています。
- 独立分布の乗算を計算する
- 必要なトレーニングデータが少ない
- 配布要件なし
- 条件付き独立仮定の下で、識別モデルよりも速く収束する（例：ロジスティック回帰）
- カテゴリ変数に適している
- 多重共線性を被る

単純ベイズ（ナイーブベイズ)
概要： 各要素(説明変数)が独立に予測対象に影響を与えていると仮定して、ベイズの定理を活用し、最もその分類が発生する確率が高いものを予測とする手法。主に文章に利用され、文章の単語をベースにした分類に使われる。
予測対象: 分類
可読性： ×
並列処理： ×
過学習防止策： 説明変数の集約(同じ意味の単語の集約など)など

## Neural Network

ニューラルネットワーク
多数の入力特徴を持つ非線形データをモデル化するのに適しています
産業で広く利用された
多くのオープンソース実装
数値入力、定数の値を持つベクトル、および欠損のないデータを持つデータセットに対してのみ。
「ブラックボックス-y」、分類境界は直感的に理解するのが難しい（「私たちの意識的な行動の背後にある理由で無意識のうちに人間に質問しようとするような」
計算コストが高い。
訓練されたモデルは初期パラメータに大きく依存します
期待通りに動作しない場合のトラブルシューティングが困難
彼らが訓練セットにないデータにうまく一般化するかどうかわからない
多層ニューラルネットワークは通常訓練が難しく、多くのパラメータを調整する必要があります。
より統計的なものやベイジアンのものとは異なり、確率的ではありません。連続数出力（例えばスコア）は、それを確率に変換することが困難であり得る。

good to model the non-linear data with large number of input features
widely used in industry
many open source implementations
only for numerical inputs, vectors with constant number of values, and datasets with non-missing data.
"black box-y", the classification boundaries are hard to understand intuitively("like trying interrogate the human unconscious for the reasons behind our conscious actions.")
computationally expensive.
the trained model depends crucially on initial parameters
difficult to troubleshoot when they don't work as expect
not sure if they will generalize well to data not in training set
multi-layer neural networks are usually hard to train, and require tuning lots of parameters
not probabilistic, unlike their more statistical or Bayesian counterparts. The continuous number output (e.g. a score) can be difficult to translate that into a probability.

## 畳み込みニューラルネットワーク(CNN)

長所：

複数のGPU間で並列化できるため、トレーニングが非常に高速です。
低レベルおよび高レベルの機能を学びますので、他のMLアルゴリズムの機能学習器として使用できます。
画像分類と言語処理を含む他の多くのタスクにおいて非常に正確です。
短所：

多くのトレーニングデータを必要とします（ただし、転送学習などの手法ではデータの必要性を減らすことができます）。
パラメータの初期化とハイパーパラメータの選択に敏感です。
CPUのトレーニングが遅い。
理論的根拠のないブラックボックス（2018年現在）。

概要： 畳み込み層とプーリング層からなるニューラルネットワークの手法。前段の層では、情報の圧縮(次元集約)を行い、後段の層では特徴抽出を行うイメージ。現状では、特に画像において有用な結果が現在示されている。
予測対象: 連続値、分類
可読性： ×
並列処理： ○
過学習防止策： Weight decay, DropOutなど非常に多い



## LSTM

長所：

比較的長い（最大40-50トークン）シーケンスを適切に分類します。
いくつかの隠れ層があっても非常に正確です。
短所：

複数のGPU間で並列化できないため、トレーニングが非常に遅くなります。
一度に1つのトークンを分類するので分類が比較的遅くなります（特にEncoder-Decoderアーキテクチャ）
理論的根拠のないブラックボックス。

Pros:

Classify relatively long (up to 40-50 tokens) sequences well;
Highly accurate even with a couple of hidden layers.

Cons:

Very slow at training as cannot be parallelized between multiple GPUs;
Relatively slow at classification, since it classifies one token at a time (especially the Encoder-Decoder architecture);
Black boxes without strong theoretical foundations.

再起型ニューラルネットワーク(RNN)
概要：連続的な情報の入力に対応したニューラルネットワークの手法。ニューラルネットワークの内部にMemory(過去の入力による影響を保持する)によって、連続的な情報に対応している。現状では、特に文章において有用な結果が示されている。
予測対象: 連続値、分類
可読性： ×
並列処理： ○
過学習防止策： Weight decay, DropOutなど非常に多い



## クラスタリング

K最近傍
K-Nearest Neighborsは、最も古く最も単純な機械学習のアルゴリズムの1つです。実際には、それほど多くの学習は行われていません。予測は、入力のすぐ近くにあるラベルの大多数（分類）または平均（回帰）として示されます（注釈なしの例）。 KNNは最近非常に正確になる可能性がありますが、予測時間が遅いと長い間考えられていました。しかし、最近のライブラリはこのアルゴリズムを非常に高速にし、今日では最先端の技術と容易に競合する可能性があります。

長所：

データについての仮定はありません。非線形に分離可能なデータに適しています。
実装が簡単です。
機能/距離の選択に柔軟に対応。
当然、マルチクラスのケースを扱います。
十分な代表的データで実際にはうまくいく
分類と回帰の両方に使用できます。
短所：

例の広いスペースを検索して最近傍を探すのは遅くなることがあります（ほとんどの場合現代のライブラリで解決されます）。
多くのトレーニング例では、モデルは多くのメモリを必要とする可能性があります。
無関係な機能やデータの規模に敏感です。

k近傍法(KNN)
概要: データを分類する手法。指定した説明変数からデータ間の距離を計算し、分類するデータに近いk個(入力値)の分類内容から最も多い分類を採用する。正解データが密である場合や傾向がドラスティックに変わらないケースに有効である。
予測対象: 分類
可読性： ○
並列処理： ○
過学習防止策：k数を大きくするなど



# クラスタリングと次元削減
## K平均
K-meansアルゴリズムは、その単純さにもかかわらず、最も使用されているクラスタリングアルゴリズムの1つです。

長所：

使い方は簡単です。
高速（空間的局所性を維持する例を格納するために適切なデータ構造とともに使用される場合）。
結果は人間にとって理解しやすいものです。
短所：

K-meansは、クラスタの形状が超球形の場合、またはクラスタが互いに離れている場合にのみ有効です。
アルゴリズムの実行が異なると、おそらく同じデータの異なるクラスタリングが得られます。
クラスタ数を事前に知っておく必要があります。



## Greedy

Greedy Algorithm （貪欲アルゴリズム、よくばり法）は、各計算ステップで、局所的に最適（その時点で最も最適と思われるもの）な選択を繰り返し、最終的に大域的な最適解を求める方法です。

貪欲法と言ってもかなり幅が広いのですが、多少強引にまとめると、「適当な基準を用いて、局所的に最適なケースを連続して選択する」だけのアルゴリズムです。言い換えれば、目先の解が最適なものであればよいという文字通り貪欲な探索手法であるといえます。

貪欲なアルゴリズムは、どの次のステップが最も明白な利益をもたらすかを決定することによって、複雑で多段階の問題に対する単純で実装しやすい解決策を探す数学的プロセスです。

このようなアルゴリズムは、貪欲と呼ばれます。
なぜなら、小さいインスタンスのそれぞれに対する最適な解決策が即座の出力を提供する一方で、アルゴリズムが全体として大きな問題を考慮していないからです。
決定がなされたら、それは再考されることはありません。


## バッチ学習 vs. オンライン学習

逐次学習 (sequential learning) †

## 統計モデルを使うのがいい場合は以下のようなときです。（説明・傾向）

不確実性を備えていて、ノイズに対するシグナルの割合が大きくない場合。（例えば、双子がいたとして、その一人は大腸癌になるかもしれず、もう一人はならないかもしれない。）そうした結果にランダム性があるような場合は、Classification（分類）をするのではなく、傾向をモデルすべきです。
完璧なトレーニングデータを持っていない時。対象となるものを何度もテストできない場合。結果をエラーなしに評価できない場合。
少ない数の変数の影響度を切り離して評価したい場合。
全体的な予測の不確実性や予測変数の影響度を調べる必要がある場合。
足し上げていくことが予測変数の結果に与える影響の主な方法である場合。交互作用が比較的小さく、それは事前に定義できるような場合。
データのサンプルサイズが大きくない場合。
例えば治療、施策、リスク要因といった特別な変数の影響を分けて考えたい場合。
モデル全体を解釈したい場合。

## 機械学習モデルを選んだほうがいい場合（判断・結果）

ノイズからシグナルを切り離しやすい場合。予測される結果が強いランダム性を持たない場合。機械学習が得意とする画像認識では、例えば対象となる文字が「E」なのか「E」でないのかがはっきりしている。
アルゴリズムが膨大な量の全く同じ結果が繰り返されるデータで学習することができる場合。（アルファベットのそれぞれの文字が1000回繰り返されるような場合）
全体的な予測がゴールであって、治療、施策といった一つの変数の影響は問題ではない場合。
予測における不確実性を推測することや選択された予測変数の影響度にはあまり興味が無い場合。
データのサンプルサイズが大きい。
モデルがブラックボックスで中で何が起きているかには興味が無い場合。


実際のビジネスでは、こういったシグナル・ノイズ・レイシオ（割合）が低いデータばかり
全く同じ説明変数を持つのに、目的変数が異なる


References

- []()
- [](https://www.hackingnote.com/en/machine-learning/algorithms-pros-and-cons)
- [](https://semanti.ca/blog/?the-most-important-machine-learning-algorithms)
- [](https://www.slideshare.net/LuMa921/deep-learning-the-past-present-and-future-of-artificial-intelligence?from_action=save)
- [](https://exploratory.io/note/kanaugust/Gxe3giQ0zp)
